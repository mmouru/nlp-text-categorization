{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Write a script that computes the FastText embedding vector of each abstract as well as the Fasttext embedding of the topic description\n",
    "\n",
    "Examples gotten from: https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "embedding_size = 2\n",
    "window_size = 10\n",
    "min_word = 5\n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Musculoskeletal Radiology\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['artificial', 'intelligence', 'advanced', 'technology', 'present']]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [58], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m word_tokenized_corpus \u001b[39m=\u001b[39m [word_punctuation_tokenizer\u001b[39m.\u001b[39mtokenize(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m lol]\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(word_tokenized_corpus)\n\u001b[1;32m----> 8\u001b[0m ft_model \u001b[39m=\u001b[39m FastText(word_tokenized_corpus,\n\u001b[0;32m      9\u001b[0m                       \u001b[39m#size=embedding_size,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m                       window\u001b[39m=\u001b[39;49mwindow_size,\n\u001b[0;32m     11\u001b[0m                       min_count\u001b[39m=\u001b[39;49mmin_word,\n\u001b[0;32m     12\u001b[0m                       sample\u001b[39m=\u001b[39;49mdown_sampling,\n\u001b[0;32m     13\u001b[0m                       sg\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\models\\fasttext.py:435\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors_vocab_lockf \u001b[39m=\u001b[39m ones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors_ngrams_lockf \u001b[39m=\u001b[39m ones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)\n\u001b[1;32m--> 435\u001b[0m \u001b[39msuper\u001b[39;49m(FastText, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    436\u001b[0m     sentences\u001b[39m=\u001b[39;49msentences, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, workers\u001b[39m=\u001b[39;49mworkers, vector_size\u001b[39m=\u001b[39;49mvector_size, epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    437\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks, batch_words\u001b[39m=\u001b[39;49mbatch_words, trim_rule\u001b[39m=\u001b[39;49mtrim_rule, sg\u001b[39m=\u001b[39;49msg, alpha\u001b[39m=\u001b[39;49malpha, window\u001b[39m=\u001b[39;49mwindow,\n\u001b[0;32m    438\u001b[0m     max_vocab_size\u001b[39m=\u001b[39;49mmax_vocab_size, max_final_vocab\u001b[39m=\u001b[39;49mmax_final_vocab,\n\u001b[0;32m    439\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count, sample\u001b[39m=\u001b[39;49msample, sorted_vocab\u001b[39m=\u001b[39;49msorted_vocab,\n\u001b[0;32m    440\u001b[0m     null_word\u001b[39m=\u001b[39;49mnull_word, ns_exponent\u001b[39m=\u001b[39;49mns_exponent, hashfxn\u001b[39m=\u001b[39;49mhashfxn,\n\u001b[0;32m    441\u001b[0m     seed\u001b[39m=\u001b[39;49mseed, hs\u001b[39m=\u001b[39;49mhs, negative\u001b[39m=\u001b[39;49mnegative, cbow_mean\u001b[39m=\u001b[39;49mcbow_mean,\n\u001b[0;32m    442\u001b[0m     min_alpha\u001b[39m=\u001b[39;49mmin_alpha, shrink_windows\u001b[39m=\u001b[39;49mshrink_windows)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\models\\word2vec.py:427\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    426\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 427\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    428\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    429\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    430\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\models\\word2vec.py:1042\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[0;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m-> 1042\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_training_sanity(epochs\u001b[39m=\u001b[39;49mepochs, total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words)\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m   1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1046\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1047\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1051\u001b[0m     ),\n\u001b[0;32m   1052\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\models\\word2vec.py:1540\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1540\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[0;32m   1542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in lol]\n",
    "\n",
    "print(word_tokenized_corpus)\n",
    "\n",
    "ft_model = FastText(word_tokenized_corpus,\n",
    "                      #size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1)\n",
    "                      #iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0, 'n': 1, 'i': 2}\n",
      "{'artificial': ['i', 'n', 'e'], 'intelligence': ['n', 'i', 'e'], 'machine': ['n', 'e', 'i'], 'network': ['e', 'i', 'n'], 'recurrent': ['i', 'n', 'e'], 'deep': ['n', 'e', 'i']}\n",
      "artificial:['i', 'n', 'e']\n",
      "intelligence:['n', 'i', 'e']\n",
      "machine:['n', 'e', 'i']\n",
      "network:['e', 'i', 'n']\n",
      "recurrent:['i', 'n', 'e']\n",
      "deep:['n', 'e', 'i']\n",
      "['artificial', 'i', 'n', 'e', 'intelligence', 'n', 'i', 'e', 'machine', 'n', 'e', 'i', 'network', 'e', 'i', 'n', 'recurrent', 'i', 'n', 'e', 'deep', 'n', 'e', 'i']\n",
      "<class 'list'>\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "test = ['artificial', 'intelligence', 'machine', 'network', 'recurrent', 'deep']\n",
    "\n",
    "print(ft_model.wv.key_to_index)\n",
    "\n",
    "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                            for words in test}\n",
    "\n",
    "print(semantically_similar_words)\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))\n",
    "\n",
    "all_similar_words = sum([[k] + v for k, v in semantically_similar_words.items()], [])\n",
    "\n",
    "print(all_similar_words)\n",
    "print(type(all_similar_words))\n",
    "print(len(all_similar_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
